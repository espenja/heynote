// This file was generated by lezer-generator. You probably shouldn't edit it.
import {LRParser} from "@lezer/lr"
import {noteContent} from "./external-tokens.js"
export const parser = LRParser.deserialize({
  version: 14,
  states: "!jQQOPOOOVOPO'#C`O[OQO'#C_OOOO'#Cc'#CcQQOPOOOaOPO,58zOOOO,58y,58yOOOO-E6a-E6aOfOPO1G.fOOOQ7+$Q7+$QOnOPO7+$QOOOQ<<Gl<<Gl",
  stateData: "s~OXPO~OYTO~OPUO~OTWO~OUYOXXO~OXZO~O",
  goto: "gWPPPX]PPaTROSTQOSQSORVS",
  nodeNames: "âš  NoteContent Document Note NoteDelimiter NoteLanguage Auto",
  maxTerm: 10,
  skippedNodes: [0],
  repeatNodeCount: 1,
  tokenData: "+[~R`YZ!T}!O!Y#V#W!e#X#Y$R#Z#[$q#[#]$w#^#_%Z#`#a&w#a#b'a#d#e(`#f#g({#g#h)b#h#i)w#l#m$}#m#n*s%&x%&y*y~!YOX~~!]P#T#U!`~!eOU~~!hR#`#a!q#d#e#f#g#h#l~!tP#c#d!w~!zP#^#_!}~#QP#i#j#T~#WP#f#g#Z~#^P#X#Y#a~#fOT~~#iP#d#e#a~#oQ#[#]#u#g#h#a~#xP#T#U#{~$OP#f#g#f~$UP#f#g$X~$[P#`#a$_~$bP#T#U$e~$hP#b#c$k~$nP#Z#[#a~$tP#c#d$X~$zP#h#i$}~%QP#a#b%T~%WP#`#a#a~%^Q#T#U%d#g#h&h~%gP#j#k%j~%mP#T#U%p~%uPT~#g#h%x~%{P#V#W&O~&RP#f#g&U~&XP#]#^&[~&_P#d#e&b~&eP#h#i#a~&kQ#c#d&q#l#m#a~&tP#b#c#a~&zP#X#Y&}~'QP#n#o'T~'WP#X#Y'Z~'^P#f#g#a~'dP#T#U'g~'jQ#f#g'p#h#i(Y~'sP#_#`'v~'yP#W#X'|~(PP#c#d(S~(VP#k#l&q~(]P#[#]#a~(cQ#[#]#f#m#n(i~(lP#h#i(o~(rP#[#](u~(xP#c#d&q~)OP#i#j)R~)UQ#U#V)[#g#h&b~)_P#m#n#a~)eQ#[#])k#e#f%T~)nP#X#Y)q~)tP#`#a%T~)zR#X#Y*T#g#h*Z#m#n*a~*WP#l#m&b~*^P#l#m#a~*dP#d#e*g~*jP#X#Y*m~*pP#g#h%x~*vP#T#U$}~*|P%&x%&y+P~+SP%&x%&y+V~+[OY~",
  tokenizers: [0, noteContent],
  topRules: {"Document":[0,2]},
  tokenPrec: 0
})
